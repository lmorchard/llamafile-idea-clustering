{
  "name": "llamafile-idea-clustering",
  "version": "1.0.0",
  "description": "",
  "main": "server.js",
  "scripts": {
    "start": "npm-run-all --parallel start:unix:*",
    "start:unix:ui-dev": "nodemon --ignore public/ ./server.js",
    "start:unix:llamafile": "./TinyLlama-1.1B-Chat-v1.0.Q4_0.llamafile -ngl 9999 --embedding --server --nobrowser --port 8887",
    "setup": "npm-run-all setup:unix:*",
    "setup:unix:llamafile": "curl --no-clobber -LO https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q4_0.llamafile?download=true && chmod +x ./TinyLlama-1.1B-Chat-v1.0.Q4_0.llamafile",
    "start:windows": "npm-run-all --parallel start:windows:*",
    "start:windows:ui-dev": "nodemon --ignore public/ ./server.js",
    "start:windows:llamafile": "TinyLlama-1.1B-Chat-v1.0.Q4_0.llamafile.exe -ngl 9999 --embedding --server --nobrowser --port 8887",
    "setup:windows": "npm-run-all setup:unix:llamafile setup:windows:*",
    "setup:windows:llamafileexe": "ren TinyLlama-1.1B-Chat-v1.0.Q4_0.llamafile TinyLlama-1.1B-Chat-v1.0.Q4_0.llamafile.exe"
  },
  "author": "",
  "license": "ISC",
  "dependencies": {
    "connect-livereload": "^0.6.1",
    "express": "^4.19.2",
    "http-proxy-middleware": "^3.0.0",
    "livereload": "^0.9.3",
    "nodemon": "^3.1.0",
    "npm-run-all": "^4.1.5"
  }
}
